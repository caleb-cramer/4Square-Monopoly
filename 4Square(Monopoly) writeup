\documentclass{article}
\usepackage{amsthm, amssymb, amsmath}

\newtheorem{theorem}{Theorem}
\newtheorem{goal}{Goal}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{subs}{Subsection}

\title{4 Square Game}
\author{Caleb Cramer}

\begin{document}
\maketitle

\section{Rules and Background}
The rules are as follows, the board is a 2 x 2 with 4 squares. The players move clockwise around the board. The only exception is that square 4 sends the player straight to square 2. The players move by spinning a spinner with two options. The players can either move 1 spot or 2 clockwise. All players start on 1 but, as we will prove later, it does not matter in the long run where the players start.

\section{Solution}
\begin{definition}
A $n$ x $n$ matrix is diagonalizable if and only if it has $n$ independent eigenvectors.
\end{definition}
\begin{lemma}
If a matrix is diagonalizable, then the nth of said matrix power can be written as $A^n = (PDP^{-1})^n = PD^nP^{-1}$
\end{lemma}
\begin{goal}
The goal of this article is to find what the long run probabilities of being on each square. This can be done with Markov chains where each state is the the square that the player is in.
\end{goal}

\begin{proof}
The probability matrix of a player moving from j to i is shown as $p_{ij}$ below. Since it is impossible for the player to ever land on the $4^{th}$ square, instead of this requiring a 4 x 4 matrix we can get rid of all entries involving those rows and columns. This pares down the matrix to a 3 x 3.
$$\begin{matrix}
p_{11} = 0 & p_{21} = 0.5 & p_{31} = 0.5\\
p_{12} = 0 & p_{22} = 0.5 & p_{32} = 0.5\\
p_{13} = 0.5 & p_{23} = 0.5 & p_{33} = 0
\end{matrix}$$

The transpose of this is matrix $A= 
\begin{bmatrix}
0 & 0 & 0.5\\0.5 & 0.5 & 0.5\\0.5 & 0.5 & 0
\end{bmatrix}$. Since we know that the players start on the first square, the initial placement vector, $v_{0}$, is $\begin{bmatrix}
1\\0\\0
\end{bmatrix}$. So after the $1^{st}$ turn the probability of being at any location can be modeled as $Av_0=\begin{bmatrix}
0\\0.5\\0.5
\end{bmatrix}$ If we continue multiplying $v_0$ by $A$, then we can display probability of the $n^{th}$ turn as $A^nv_0 = \begin{bmatrix}
p_1\\
p_2\\
p_3
\end{bmatrix}$ where $p_1, p_2, p_3$ are the probability of landing on squares 1, 2, and 3, respectively. We can find $A^n$ by the method of diagonalization. To do this we must find $P, D,$ and $P^{-1}$ where $P$ is constructed from the eigenvectors and $D$ is based on eigenvalues.

\paragraph{Solving for Eigenvectors}
To find eigenvalues, start with:
$$(A-I_n\lambda) = \begin{bmatrix}
-\lambda & 0 & 0.5\\
0.5 & 0.5-\lambda & 0.5\\
0.5 & 0.5 & -\lambda
\end{bmatrix}$$
Then the determinant can be found with a cofactor expansion along the first row:
$$det(A-I_n\lambda) = -\lambda[(-\lambda)(0.5-\lambda)-0.25] - 0 + 0.5[0.25-(0.5)(0.5-\lambda)]$$
The eigenvalues resulting from this are $\lambda = 0, 1, -0.5$.\\
\boldmath{$\lambda = 0$}\unboldmath{}
$$\begin{bmatrix}
0 & 0 & 0.5\\
0.5 & 0.5 & 0.5\\
0.5 & 0.5 & 0
\end{bmatrix}$$
And after some simple row swaps and adding multiples of one row to another, we arrive at:
$$\begin{bmatrix}
0.5 & 0.5 & 0.5\\
0 & 0 & 0.5\\
0 & 0 & 0
\end{bmatrix}$$

So we can assume that the third column is free, since it has no pivot. Then we can build our eigenvector around that. A simple vector that fulfills all the requirements is 
$\begin{bmatrix}-1 \\1 \\0
\end{bmatrix}$.\\
\boldmath{$\lambda = 1$}\unboldmath{}
$$\begin{bmatrix}
-1 & 0 & 0.5\\
0.5 & -0.5 & 0.5\\
0.5 & 0.5 & -1
\end{bmatrix}$$
Following the same processes as above, we end up at:
$$\begin{bmatrix}
-1 & 0 & 0.5\\
0 & -0.5 & 0.75\\
0 & 0 & 0
\end{bmatrix}$$
Building the eigenvector in the same was we find that $\begin{bmatrix}
1 \\
3 \\
2
\end{bmatrix}$, when multiplied by the previous matrix, gives the 0 vector.\\
\boldmath{$\lambda = -0.5$}\unboldmath{}
$$\begin{bmatrix}
0.5 & 0 & 0.5\\
0.5 & 0 & 0.5\\
0.5 & 0.5 & 0.5
\end{bmatrix}$$
Which becomes,
$$\begin{bmatrix}
0.5 & 0.5 & 0.5\\
0.5 & 0 & 0.5\\
0 & 0 & 0
\end{bmatrix}$$
The final vector that fulfills all the requirements is $\begin{bmatrix}
-1 \\
0 \\
1
\end{bmatrix}$.
\paragraph{Solving for Diagonalization Matrix}
By Definition 1, we know that $A$ is diagonalizable. Therefore by Lemma 1, $A^n$ can be written as a product of $P, D^{n}, P^{-1}$.\\
The matrix $D^{n}$ is made by placing the eigenvalues on the main diagonal in any order and then multiplying that matrix by itself until it is equal to $n$. Because of the properties of a diagonal matrix, this can be generalized by raising the diagonal components by $n$:
$$\begin{bmatrix}
1^{n} & 0 & 0\\
0 & 0^{n} & 0\\
0 & 0 & -0.5^{n}
\end{bmatrix}$$
The matrix $P$ requires that we place the eigenvectors in the same order we placed their corresponding eigenvalues. Matrix $P$ looks like:
$$\begin{bmatrix}
1 & -1 & -1\\
3 & 1 & 0\\
2 & 0 & 1
\end{bmatrix}$$
And $P^{-1}$ is simply the inverse matrix of $P$. So $P^{-1}$ is:
$$\begin{bmatrix}
\frac{1}{6} & \frac{1}{6} & \frac{1}{6}\\
-\frac{1}{2} & \frac{1}{2} & -\frac{1}{2}\\
-\frac{1}{3} & -\frac{1}{3} & -\frac{2}{3}\\
\end{bmatrix}$$

Continuing on with the proof,
$$A^{n} = \begin{bmatrix}
1^{n} & 0 & 0\\
0 & 0^{n} & 0\\
0 & 0 & -0.5^{n}
\end{bmatrix}
\begin{bmatrix}
1 & -1 & -1\\
3 & 1 & 0\\
2 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
\frac{1}{6} & \frac{1}{6} & \frac{1}{6}\\
-\frac{1}{2} & \frac{1}{2} & -\frac{1}{2}\\
-\frac{1}{3} & -\frac{1}{3} & -\frac{2}{3}\\
\end{bmatrix}$$

$$= \begin{bmatrix}
0.1\overline{6}-0.\overline{3}(0.5)^{n} & 0.1\overline{6}-0.\overline{3}(0.5)^{n} & 0.1\overline{6}-0.\overline{6}(0.5)^{n}\\
0.5 & 0.5 & 0.5\\
0.\overline{3}+0.\overline{3}(0.5)^{n} & 0.\overline{3}+0.\overline{3}(0.5)^{n} & 0.\overline{3}+0.\overline{6}(0.5)^{n}
\end{bmatrix}$$
\paragraph{Results}
To find the long run probabilities of landing on any particular square, we just need to simulate playing the game forever. Sounds simple enough. Since we know the probabilities after any $n$ turn, we just need to find the limit as $n\to\infty$. Therefore $$\lim_{n\to\infty} A^{n} = \begin{bmatrix}
0.1\overline{6}-0 & 0.1\overline{6}-0 & 0.1\overline{6}-0\\
0.5 & 0.5 & 0.5\\
0.\overline{3}+0 & 0.\overline{3}+0 & 0.\overline{3}+0
\end{bmatrix}$$
Going back to the first paragraph, if we multiply this by $v_0$ we get the probability vector of $\begin{vmatrix}
0.1\overline{6}\\
0.5\\
0.\overline{3}
\end{vmatrix}$. Interestingly enough, it does not matter where the players start because the terms will all be the same regardless of starting on square 1, 2, or 3.
\paragraph{Conclusion}
This means that the long run probability of landing on square 1 is 16.6\%, the probability of landing on square 2 is 50\%, the probability of landing on square 3 is 33.3\% and the probability of landing on square 4 is 0\%.
\end{proof}



\end{document}
